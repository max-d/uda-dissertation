# -*- coding: utf-8 -*-
"""step5.256.unet-new-evaluate-dataset2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rM2MLbgoqNCa8_m9tm7OFUo3VtXccQTY

# Evaluating fine tuned model on Dataset B
"""

!pip install ftw-tools

import tensorflow as tf
import numpy as np

print(f"TensorFlow Version: {tf.__version__}")
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        print(f"GPUs detected: {gpus}")
        # Attempt to set memory growth for the first GPU
        tf.config.experimental.set_memory_growth(gpus[0], True)
        print("Successfully set memory growth for GPU 0.")
    except RuntimeError as e:
        print(f"!!! Could not set memory growth (GPU likely already initialized?): {e}")
    except Exception as e:
        print(f"!!! An unexpected error occurred during GPU configuration: {e}")

    try:
         with tf.device('/device:GPU:0'):
             a = tf.constant([[1.0]], dtype=tf.float32)
             b = tf.constant([[2.0]], dtype=tf.float32)
             c = tf.matmul(a, b)
         print("Minimal GPU computation successful after setting memory growth.")
    except Exception as e:
         print(f"!!! Error during minimal GPU computation: {e}")

else:
    print("!!! No GPU detected by TensorFlow.")

!pip install tifffile imagecodecs

"""# #Mount Google Drive

"""

from google.colab import drive
drive.mount("/content/drive")

!ftw data download --countries spain

!ls -la /content/data/spain.zip

!ls -la ./drive/MyDrive/dissertation/datasets/

!cp /content/data/spain.zip ./drive/MyDrive/dissertation/datasets/spain.zip

# check GPU
gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Not connected to a GPU')
else:
  print(gpu_info)

import os

# import libs for processing images
import imageio
import imagecodecs
from PIL import Image

# import visualizations
import matplotlib.pyplot as plt

import numpy as np # for using np arrays

# TF and co
import tensorflow as tf
from tensorflow.keras.layers import Input
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import MaxPooling2D
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.layers import Conv2DTranspose
from tensorflow.keras.layers import concatenate
from tensorflow.keras.losses import binary_crossentropy
from sklearn.model_selection import train_test_split

import os
import tifffile
import numpy as np
import imagecodecs
import rasterio

def LoadAndProcessData(image_dir, mask_dir, mask_channel_index=0, mask_dtype=np.uint8):
    try:

        image_filenames = sorted(os.listdir(image_dir))
        mask_filenames = sorted(os.listdir(mask_dir))

        if len(image_filenames) != len(mask_filenames):
            print(f"Warning: Mismatch in file count! Images: {len(image_filenames)}, Masks: {len(mask_filenames)}")

        print(f"Found {len(image_filenames)} potential image/mask pairs.")

        loaded_images = []
        processed_masks = []

        for img_fname, mask_fname in zip(image_filenames, mask_filenames):
            img_base = os.path.splitext(img_fname)[0]
            mask_base = os.path.splitext(mask_fname)[0]

            img_full_path = os.path.join(image_dir, img_fname)
            mask_full_path = os.path.join(mask_dir, mask_fname)

            try:
                with rasterio.open(img_full_path) as src:
                    window_a = src.read()[0:3, :, :]  # Reading first 3 bands
                    image_data = window_a.transpose(1, 2, 0) / 3000  # Normalizing


                with rasterio.open(mask_full_path) as src:
                    semantic_2_class = src.read()

                multi_channel_mask_data = semantic_2_class

                if multi_channel_mask_data.ndim == 3 and multi_channel_mask_data.shape[2] > mask_channel_index:
                    raw_mask = multi_channel_mask_data[:, :, mask_channel_index]
                elif multi_channel_mask_data.ndim == 2 and mask_channel_index == 0:

                    print(f"  Info: Mask '{mask_fname}' is already 2D, using as is.")
                    raw_mask = multi_channel_mask_data
                else:

                    print(f"Error: Mask '{mask_fname}' has unexpected shape {multi_channel_mask_data.shape} or invalid channel index {mask_channel_index}. Skipping pair.")
                    continue

                final_mask = raw_mask.astype(mask_dtype)

                loaded_images.append(image_data)
                processed_masks.append(semantic_2_class[0])

            except FileNotFoundError:
                print(f"Error: File not found for pair. Image: '{img_full_path}', Mask: '{mask_full_path}'. Skipping.")
            except Exception as e:
                print(f"Error processing pair: Image='{img_fname}', Mask='{mask_fname}'. Error: {e}. Skipping.")

    except FileNotFoundError as e:
        print(f"Error: Cannot access directory. Path: {e.filename}")
        return [], []
    except Exception as e:
        print(f"An unexpected error occurred in LoadAndProcessData: {e}")
        return [], []

    print(f"Successfully loaded {len(loaded_images)} image/mask pairs.")
    return loaded_images, processed_masks

import cv2
import numpy as np
from PIL import Image

def PreprocessData(loaded_images, processed_masks, target_shape_img, target_shape_mask):
    m = len(loaded_images)
    if m == 0:
        print("Warning: No images/masks provided.")
        return np.zeros((0, *target_shape_img), dtype=np.float32), np.zeros((0, *target_shape_mask[:2], target_shape_mask[2]), dtype=np.int32)


    i_h, i_w, i_c = target_shape_img
    m_h, m_w, m_c = target_shape_mask

    if m_c != 1:
         print(f"Warning: target_shape_mask expects {m_c} channels, but semantic segmentation usually requires 1.")

    X = np.zeros((m, i_h, i_w, i_c), dtype=np.float32)
    y = np.zeros((m, m_h, m_w, m_c), dtype=np.int32)

    print(f"Preprocessing {m} image/mask pairs...")

    for index, (img_array, mask_array) in enumerate(zip(loaded_images, processed_masks)):
        try:

            print(f"Input index {index}: dtype={img_array.dtype}, shape={img_array.shape}, min={np.min(img_array)}, max={np.max(img_array)}")

            img_array_f32 = img_array.astype(np.float32)

            if img_array_f32.ndim == 2:
                 img_array_f32 = cv2.cvtColor(img_array_f32, cv2.COLOR_GRAY2RGB)
            elif img_array_f32.shape[2] == 1:
                 img_array_f32 = cv2.cvtColor(img_array_f32, cv2.COLOR_GRAY2RGB)

            if img_array_f32.shape[0] != i_h or img_array_f32.shape[1] != i_w:
                 img_resized_array = cv2.resize(img_array_f32, (i_w, i_h), interpolation=cv2.INTER_LINEAR)
            else:
                 img_resized_array = img_array_f32 # No resize needed

            scale_divisor = 2.5
            scaled_image = img_resized_array / scale_divisor

            scaled_image = np.clip(scaled_image, 0.0, 1.0)

            X[index] = scaled_image # Already float32

            #Process Mask
            if mask_array.ndim == 3 and mask_array.shape[2] == 1:
                mask_array = np.squeeze(mask_array, axis=-1) # Remove channel dim
            if mask_array.ndim != 2:
                 raise ValueError(f"Mask at index {index} has unexpected shape {mask_array.shape} after squeeze.")


            if mask_array.dtype != np.uint8:
                 print(f"Warning: Mask {index} dtype is {mask_array.dtype}. Converting to uint8. Max label: {np.max(mask_array)}")
                 if np.max(mask_array) > 255:
                     print(f"ERROR: Mask {index} has labels > 255, cannot safely convert to uint8 for PIL.")
                     continue
                 mask_array = mask_array.astype(np.uint8)

            pil_mask = Image.fromarray(mask_array)
            # Resize mask using NEAREST
            if pil_mask.size != (m_w, m_h):
                 pil_mask_resized = pil_mask.resize((m_w, m_h), Image.Resampling.NEAREST)
            else:
                 pil_mask_resized = pil_mask

            mask_resized_array = np.array(pil_mask_resized) #

            y[index] = np.expand_dims(mask_resized_array, axis=-1).astype(np.int32)

        except Exception as e:
            print(f"Error processing data at index {index}: {e}")

            X[index] = 0
            y[index] = -1


    print("Finished preprocessing.")

    return X, y

import os
import matplotlib.pyplot as plt
import numpy as np
import tifffile
import imagecodecs

IMAGE_TRAIN_PATH = './data/ftw/spain/s2_images/window_b'
MASK_TRAIN_PATH = './data/ftw/spain/label_masks/semantic_2class/'

if os.path.exists(IMAGE_TRAIN_PATH) and os.path.exists(MASK_TRAIN_PATH):
    print("Loading data using LoadAndProcessData...")
    img, mask = LoadAndProcessData(IMAGE_TRAIN_PATH, MASK_TRAIN_PATH, mask_channel_index=0, mask_dtype=np.uint8)
else:
    print(f"Error: Training directories not found at '{IMAGE_TRAIN_PATH}' or '{MASK_TRAIN_PATH}'")
    img, mask = [], [] #

if img and mask:

    show_images = min(5, len(img))
    print(f"\nDisplaying {show_images} example(s):")
    for i in range(show_images):
        img_view = img[i]
        mask_view = mask[i]

        print(f"\nImage {i}:")
        print(f"  Shape: {img_view.shape}, dtype: {img_view.dtype}")
        print(f"Mask {i}:")
        print(f"  Shape: {mask_view.shape}, dtype: {mask_view.dtype}")
        print(f"  Unique mask values: {np.unique(mask_view)}")

        fig, arr = plt.subplots(1, 2, figsize=(15, 7))
        arr[0].imshow(img_view)
        arr[0].set_title(f'Image {i}')
        arr[0].axis('off')
        arr[1].imshow(mask_view, cmap='viridis',vmin=0, vmax=2)
        arr[1].set_title(f'Processed Mask {i} (Channel 0)')
        arr[1].axis('off')

        plt.tight_layout()
        plt.show()
else:
    print("\nNo data loaded, skipping display.")

"""Prepare train dataset"""

target_shape_img = [256, 256, 3]
target_shape_mask = [256, 256, 1]


if 'img' in locals() and 'mask' in locals() and img and mask:

    X_train, y_train = PreprocessData(img, mask, target_shape_img, target_shape_mask)


    if X_train.size > 0 and y_train.size > 0 :
        total_pixels = y_train.size
        unique_classes, counts = np.unique(y_train, return_counts=True)
        print("\nClass distribution in y_train (after resizing):")
        for cls, count in zip(unique_classes, counts):
            percentage = (count / total_pixels) * 100
            print(f"  Class {cls}: {count} pixels ({percentage:.2f}%)")

        print("\nProcessed Shapes:")
        print("X_train Shape:", X_train.shape)
        print("y_train shape:", y_train.shape)

        image_index = 0
        print(f"\nVisualizing processed example index {image_index}:")
        fig, arr = plt.subplots(1, 2, figsize=(10, 5))

        print(f"Data type: {X_train[image_index].dtype}")
        print(f"Shape: {X_train[image_index].shape}")
        print(f"Min value: {np.min(X_train[image_index])}")
        print(f"Max value: {np.max(X_train[image_index])}")
        print(f"Mean value: {np.mean(X_train[image_index])}")

        arr[0].imshow(X_train[image_index])
        arr[0].set_title('Processed Image')
        arr[0].axis('off')

        arr[1].imshow(np.squeeze(y_train[image_index]), cmap='viridis')
        arr[1].set_title('Processed Mask')
        arr[1].axis('off')

        plt.tight_layout()
        plt.show()
    else:
        print("\nPreprocessing resulted in empty arrays. Check logs for errors.")
else:
    print("\nVariables 'img' and 'mask' not found or are empty. Run LoadAndProcessData first.")

print(np.unique(y_train))

"""## Load Pretrained Model

Load and evaluate pretrained model
"""

!cp ./drive/MyDrive/dissertation/unet_basic_v2.keras ./unet_basic_v2.keras

class SegF1(tf.keras.metrics.Metric):
    def __init__(self, name="macro_f1", **kw):
        super().__init__(name=name, **kw)
        self.f1 = tf.keras.metrics.F1Score(average="macro", threshold=None)

    def update_state(self, y_true, y_pred, sample_weight=None):
        # flatten to (N,) then expand to (N,1) â†’ now 2-D
        y_true = tf.reshape(y_true, [-1, 1])
        y_pred = tf.reshape(tf.argmax(y_pred, -1), [-1, 1])
        self.f1.update_state(y_true, y_pred, sample_weight)

    def result(self):       return self.f1.result()
    def reset_states(self): self.f1.reset_states()

# Cell 6: Evaluate (CPU) - Corrected Version
if 'X_train' in locals() and 'y_train' in locals():
    print("Starting CPU evaluation...")
    try:
        # Force operations AND variable placement onto CPU
        with tf.device('/device:CPU:0'):
            print("Loading model onto CPU...")
            # Load the model INSIDE the CPU context
            unetv2_cpu = tf.keras.models.load_model('./unet_basic_v2.keras', custom_objects={"SegF1": SegF1})
            print("Model loaded onto CPU. Starting evaluation...")
            # Evaluate using the CPU-loaded model
            results_cpu = unetv2_cpu.evaluate(X_train, y_train, batch_size=16, verbose=1)
            print("\nCPU Evaluation Results:", results_cpu)
            print("CPU evaluation completed without runtime errors.")
    except Exception as e:
        print("\n!!! Error during CPU evaluation:")
        print(e) # Note any errors here
else:
    print("Data not loaded, skipping CPU evaluation.")

"""[1.1793988943099976, 0.5151708722114563, 0.15262381732463837, 0.2648284435272217]

[0.7382933497428894, 0.6113061904907227, 0.44371557235717773, 0.6146856546401978]
"""

print(unetv2_cpu.metrics_names)

"""### Load non fonetuned model for comparison"""

!cp ./drive/MyDrive/dissertation/unet_basic_v1_256.keras ./unet_basic_v1_256.keras

# Cell 6: Evaluate (CPU) - Corrected Version
if 'X_train' in locals() and 'y_train' in locals():
    print("Starting CPU evaluation...")
    try:
        # Force operations AND variable placement onto CPU
        with tf.device('/device:CPU:0'):
            print("Loading model onto CPU...")
            # Load the model INSIDE the CPU context
            unetv1_cpu = tf.keras.models.load_model('./unet_basic_v1_256.keras')
            print("Model loaded onto CPU. Starting evaluation...")
            # Evaluate using the CPU-loaded model
            results_cpu = unetv1_cpu.evaluate(X_train, y_train, batch_size=16, verbose=1)
            print("\nCPU Evaluation Results:", results_cpu)
            print("CPU evaluation completed without runtime errors.")
    except Exception as e:
        print("\n!!! Error during CPU evaluation:")
        print(e) # Note any errors here
else:
    print("Data not loaded, skipping CPU evaluation.")

with tf.device('/device:CPU:0'):
    unetv3_cpu = tf.keras.models.load_model('./unet_basic_v2.keras', custom_objects={"SegF1": SegF1})
    unetv1_cpu = tf.keras.models.load_model('./unet_basic_v1_256.keras', custom_objects={"SegF1": SegF1})

# Results of Validation Dataset
def VisualizeResultsV3(index):
  with tf.device('/device:CPU:0'):
    img = X_train[index]
    img = img[np.newaxis, ...]
    pred_y_v3 = unetv3_cpu.predict(img)
    pred_mask_v3 = tf.argmax(pred_y_v3[0], axis=-1)
    pred_mask_v3 = pred_mask_v3[..., tf.newaxis]

    pred_y_v1 = unetv1_cpu.predict(img)
    pred_mask_v1 = tf.argmax(pred_y_v1[0], axis=-1)
    pred_mask_v1 = pred_mask_v1[..., tf.newaxis]

    fig, arr = plt.subplots(1, 4, figsize=(15, 15))
    arr[0].imshow(X_train[index])
    arr[0].set_title('Processed Image')
    arr[1].imshow(y_train[index,:,:,0])
    arr[1].set_title('Actual Masked Image ')
    arr[2].imshow(pred_mask_v1[:,:,0])
    arr[2].set_title('Predicted Masked Image U-net V1')
    arr[3].imshow(pred_mask_v3[:,:,0])
    arr[3].set_title('Predicted Masked Image U-net V2')

index = 79
VisualizeResultsV3(index)

for i in range(1000,1020):
  VisualizeResultsV3(i)